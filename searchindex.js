Search.setIndex({"alltitles": {"Bayesian UCB (bayesian_ucb.py)": [[2, "bayesian-ucb-bayesian-ucb-py"]], "Contents:": [[1, null]], "Contextual Bandits (contextual_bandits.py)": [[2, "contextual-bandits-contextual-bandits-py"]], "Epsilon Greedy (epsilon_greedy.py)": [[2, "epsilon-greedy-epsilon-greedy-py"]], "Example": [[0, null], [2, null], [2, null], [2, null], [2, null], [2, null], [2, null], [3, null], [3, null], [3, null]], "Example Usage": [[0, "example-usage"], [2, "example-usage"], [3, "example-usage"]], "Game (game.py)": [[0, "game-game-py"]], "Game documentation": [[0, null]], "Gradient (gradient.py)": [[2, "gradient-gradient-py"]], "Greedy (greedy.py)": [[2, "greedy-greedy-py"]], "Indices and tables": [[1, "indices-and-tables"]], "Policies documentation": [[2, null]], "Policy (policy.py)": [[2, "policy-policy-py"]], "PyMAB documentation": [[1, null]], "Reward Distribution (reward_distribution.py)": [[3, "reward-distribution-reward-distribution-py"]], "Reward Distribution documentation": [[3, null]], "Simple Example": [[1, "simple-example"]], "Softmax Selection (softmax_selection.py)": [[2, "softmax-selection-softmax-selection-py"]], "Thompson Sampling (thompson_sampling.py)": [[2, "thompson-sampling-thompson-sampling-py"]], "UCB (ucb.py)": [[2, "ucb-ucb-py"]]}, "docnames": ["game", "index", "policies", "reward_distribution"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1}, "filenames": ["game.rst", "index.rst", "policies.rst", "reward_distribution.rst"], "indexentries": {"__init__() (abruptchangeenvironmentmixin method)": [[0, "pymab.game.AbruptChangeEnvironmentMixin.__init__", false]], "__init__() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.__init__", false]], "__init__() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.__init__", false]], "__init__() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.__init__", false]], "__init__() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.__init__", false]], "__init__() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.__init__", false]], "__init__() (game method)": [[0, "pymab.game.Game.__init__", false]], "__init__() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.__init__", false]], "__init__() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.__init__", false]], "__init__() (gradualchangeenvironmentmixin method)": [[0, "pymab.game.GradualChangeEnvironmentMixin.__init__", false]], "__init__() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy.__init__", false]], "__init__() (policy method)": [[2, "pymab.policies.policy.Policy.__init__", false]], "__init__() (randomarmswappingenvironmentmixin method)": [[0, "pymab.game.RandomArmSwappingEnvironmentMixin.__init__", false]], "__init__() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.__init__", false]], "__init__() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.__init__", false]], "__init__() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy.__init__", false]], "__new__() (bayesianucbpolicy static method)": [[2, "pymab.policies.bayesian_ucb.BayesianUCBPolicy.__new__", false]], "__new__() (thompsonsamplingpolicy static method)": [[2, "pymab.policies.thompson_sampling.ThompsonSamplingPolicy.__new__", false]], "_calculate_confidence_interval() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy._calculate_confidence_interval", false]], "_calculate_confidence_interval() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy._calculate_confidence_interval", false]], "_calculate_confidence_interval() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy._calculate_confidence_interval", false]], "_calculate_confidence_interval() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy._calculate_confidence_interval", false]], "_calculate_confidence_interval() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy._calculate_confidence_interval", false]], "_calculate_confidence_interval() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy._calculate_confidence_interval", false]], "_get_actual_reward() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy._get_actual_reward", false]], "_get_actual_reward() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy._get_actual_reward", false]], "_get_actual_reward() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy._get_actual_reward", false]], "_get_actual_reward() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy._get_actual_reward", false]], "_get_actual_reward() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy._get_actual_reward", false]], "_get_actual_reward() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy._get_actual_reward", false]], "_get_actual_reward() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy._get_actual_reward", false]], "_get_actual_reward() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy._get_actual_reward", false]], "_get_actual_reward() (policy method)": [[2, "pymab.policies.policy.Policy._get_actual_reward", false]], "_get_actual_reward() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy._get_actual_reward", false]], "_get_actual_reward() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy._get_actual_reward", false]], "_get_actual_reward() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy._get_actual_reward", false]], "_get_ucb_value() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy._get_ucb_value", false]], "_get_ucb_value() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy._get_ucb_value", false]], "_get_ucb_value() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy._get_ucb_value", false]], "_get_ucb_value() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy._get_ucb_value", false]], "_get_ucb_value() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy._get_ucb_value", false]], "_get_ucb_value() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy._get_ucb_value", false]], "_q_values (policy attribute)": [[2, "pymab.policies.policy.Policy._Q_values", false]], "_update() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy._update", false]], "_update() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy._update", false]], "_update() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy._update", false]], "_update() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy._update", false]], "_update() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy._update", false]], "_update() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy._update", false]], "_update() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy._update", false]], "_update() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy._update", false]], "_update() (policy method)": [[2, "pymab.policies.policy.Policy._update", false]], "_update() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy._update", false]], "_update() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy._update", false]], "_update() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy._update", false]], "_update_estimate() (policy method)": [[2, "pymab.policies.policy.Policy._update_estimate", false]], "_update_sliding_window() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy._update_sliding_window", false]], "_update_sliding_window() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy._update_sliding_window", false]], "_update_sliding_window() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy._update_sliding_window", false]], "_update_sliding_window() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy._update_sliding_window", false]], "_update_sliding_window() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy._update_sliding_window", false]], "_update_sliding_window() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy._update_sliding_window", false]], "_update_sliding_window() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy._update_sliding_window", false]], "_update_sliding_window() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy._update_sliding_window", false]], "_update_sliding_window() (policy method)": [[2, "pymab.policies.policy.Policy._update_sliding_window", false]], "_update_sliding_window() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy._update_sliding_window", false]], "_update_sliding_window() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy._update_sliding_window", false]], "_update_sliding_window() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy._update_sliding_window", false]], "abrupt (environmentchangetype attribute)": [[0, "pymab.game.EnvironmentChangeType.ABRUPT", false]], "abruptchangeenvironmentmixin (class in pymab.game)": [[0, "pymab.game.AbruptChangeEnvironmentMixin", false]], "actions_estimated_reward (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.actions_estimated_reward", false]], "actions_estimated_reward (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.actions_estimated_reward", false]], "actions_estimated_reward (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.actions_estimated_reward", false]], "actions_estimated_reward (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.actions_estimated_reward", false]], "actions_estimated_reward (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.actions_estimated_reward", false]], "actions_estimated_reward (policy attribute)": [[2, "pymab.policies.policy.Policy.actions_estimated_reward", false]], "actions_estimated_reward (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.actions_estimated_reward", false]], "actions_estimated_reward (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.actions_estimated_reward", false]], "actions_estimated_reward (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.actions_estimated_reward", false]], "actions_selected_by_policy (game attribute)": [[0, "pymab.game.Game.actions_selected_by_policy", false]], "apply_change() (abruptchangeenvironmentmixin method)": [[0, "pymab.game.AbruptChangeEnvironmentMixin.apply_change", false]], "apply_change() (environmentchangemixin method)": [[0, "pymab.game.EnvironmentChangeMixin.apply_change", false]], "apply_change() (gradualchangeenvironmentmixin method)": [[0, "pymab.game.GradualChangeEnvironmentMixin.apply_change", false]], "apply_change() (randomarmswappingenvironmentmixin method)": [[0, "pymab.game.RandomArmSwappingEnvironmentMixin.apply_change", false]], "apply_change() (stationaryenvironmentmixin method)": [[0, "pymab.game.StationaryEnvironmentMixin.apply_change", false]], "average_rewards_by_episode (game property)": [[0, "pymab.game.Game.average_rewards_by_episode", false]], "average_rewards_by_step (game property)": [[0, "pymab.game.Game.average_rewards_by_step", false]], "bayesianucbpolicy (class in pymab.policies.bayesian_ucb)": [[2, "pymab.policies.bayesian_ucb.BayesianUCBPolicy", false]], "bernoullibayesianucbpolicy (class in pymab.policies.bayesian_ucb)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy", false]], "bernoullirewarddistribution (class in pymab.reward_distribution)": [[3, "pymab.reward_distribution.BernoulliRewardDistribution", false]], "bernoullithompsonsamplingpolicy (class in pymab.policies.thompson_sampling)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy", false]], "c (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.c", false]], "c (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.c", false]], "c (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.c", false]], "c (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.c", false]], "c (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.c", false]], "c (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.c", false]], "context_dim (contextualbanditpolicy attribute)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.context_dim", false]], "context_func (policy attribute)": [[2, "pymab.policies.policy.Policy.context_func", false]], "contextualbanditpolicy (class in pymab.policies.contextual_bandits)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy", false]], "cumulative_regret_by_step (game property)": [[0, "pymab.game.Game.cumulative_regret_by_step", false]], "current_step (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.current_step", false]], "current_step (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.current_step", false]], "current_step (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.current_step", false]], "current_step (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.current_step", false]], "current_step (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.current_step", false]], "current_step (policy attribute)": [[2, "pymab.policies.policy.Policy.current_step", false]], "current_step (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.current_step", false]], "current_step (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.current_step", false]], "current_step (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.current_step", false]], "discount_factor (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.discount_factor", false]], "discounteducbpolicy (class in pymab.policies.ucb)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy", false]], "effective_n (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.effective_n", false]], "environmentchangemixin (class in pymab.game)": [[0, "pymab.game.EnvironmentChangeMixin", false]], "environmentchangetype (class in pymab.game)": [[0, "pymab.game.EnvironmentChangeType", false]], "epsilon (epsilongreedypolicy attribute)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.epsilon", false]], "epsilongreedypolicy (class in pymab.policies.epsilon_greedy)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy", false]], "failures (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.failures", false]], "failures (bernoullithompsonsamplingpolicy attribute)": [[2, "id29", false], [2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.failures", false]], "game (class in pymab.game)": [[0, "pymab.game.Game", false]], "game_loop() (game method)": [[0, "pymab.game.Game.game_loop", false]], "gaussianbayesianucbpolicy (class in pymab.policies.bayesian_ucb)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy", false]], "gaussianrewarddistribution (class in pymab.reward_distribution)": [[3, "pymab.reward_distribution.GaussianRewardDistribution", false]], "gaussianthompsonsamplingpolicy (class in pymab.policies.thompson_sampling)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy", false]], "generate_q_values() (bernoullirewarddistribution static method)": [[3, "pymab.reward_distribution.BernoulliRewardDistribution.generate_Q_values", false]], "generate_q_values() (gaussianrewarddistribution static method)": [[3, "pymab.reward_distribution.GaussianRewardDistribution.generate_Q_values", false]], "generate_q_values() (rewarddistribution static method)": [[3, "pymab.reward_distribution.RewardDistribution.generate_Q_values", false]], "generate_q_values() (uniformrewarddistribution static method)": [[3, "pymab.reward_distribution.UniformRewardDistribution.generate_Q_values", false]], "get_reward() (bernoullirewarddistribution static method)": [[3, "pymab.reward_distribution.BernoulliRewardDistribution.get_reward", false]], "get_reward() (gaussianrewarddistribution static method)": [[3, "pymab.reward_distribution.GaussianRewardDistribution.get_reward", false]], "get_reward() (rewarddistribution static method)": [[3, "pymab.reward_distribution.RewardDistribution.get_reward", false]], "get_reward() (uniformrewarddistribution static method)": [[3, "pymab.reward_distribution.UniformRewardDistribution.get_reward", false]], "get_reward_distribution() (bernoullibayesianucbpolicy static method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.get_reward_distribution", false]], "get_reward_distribution() (bernoullithompsonsamplingpolicy static method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.get_reward_distribution", false]], "get_reward_distribution() (contextualbanditpolicy static method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.get_reward_distribution", false]], "get_reward_distribution() (discounteducbpolicy static method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.get_reward_distribution", false]], "get_reward_distribution() (epsilongreedypolicy static method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.get_reward_distribution", false]], "get_reward_distribution() (gaussianbayesianucbpolicy static method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.get_reward_distribution", false]], "get_reward_distribution() (gaussianthompsonsamplingpolicy static method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.get_reward_distribution", false]], "get_reward_distribution() (greedypolicy static method)": [[2, "pymab.policies.greedy.GreedyPolicy.get_reward_distribution", false]], "get_reward_distribution() (policy static method)": [[2, "pymab.policies.policy.Policy.get_reward_distribution", false]], "get_reward_distribution() (slidingwindowucbpolicy static method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.get_reward_distribution", false]], "get_reward_distribution() (stationaryucbpolicy static method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.get_reward_distribution", false]], "get_reward_distribution() (ucbpolicy static method)": [[2, "pymab.policies.ucb.UCBPolicy.get_reward_distribution", false]], "gradual (environmentchangetype attribute)": [[0, "pymab.game.EnvironmentChangeType.GRADUAL", false]], "gradualchangeenvironmentmixin (class in pymab.game)": [[0, "pymab.game.GradualChangeEnvironmentMixin", false]], "greedypolicy (class in pymab.policies.greedy)": [[2, "pymab.policies.greedy.GreedyPolicy", false]], "learning_rate (contextualbanditpolicy attribute)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.learning_rate", false]], "means (gaussianthompsonsamplingpolicy attribute)": [[2, "id38", false], [2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.means", false]], "module": [[0, "module-pymab.game", false], [2, "module-pymab.policies.bayesian_ucb", false], [2, "module-pymab.policies.contextual_bandits", false], [2, "module-pymab.policies.epsilon_greedy", false], [2, "module-pymab.policies.gradient", false], [2, "module-pymab.policies.greedy", false], [2, "module-pymab.policies.policy", false], [2, "module-pymab.policies.softmax_selection", false], [2, "module-pymab.policies.thompson_sampling", false], [2, "module-pymab.policies.ucb", false], [3, "module-pymab.reward_distribution", false]], "n_bandits (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.n_bandits", false]], "n_bandits (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.n_bandits", false]], "n_bandits (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.n_bandits", false]], "n_bandits (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.n_bandits", false]], "n_bandits (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.n_bandits", false]], "n_bandits (policy attribute)": [[2, "pymab.policies.policy.Policy.n_bandits", false]], "n_bandits (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.n_bandits", false]], "n_bandits (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.n_bandits", false]], "n_bandits (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.n_bandits", false]], "n_episodes (game attribute)": [[0, "pymab.game.Game.n_episodes", false]], "n_steps (game attribute)": [[0, "pymab.game.Game.n_steps", false]], "new_episode() (game method)": [[0, "pymab.game.Game.new_episode", false]], "no_context_func() (in module pymab.policies.policy)": [[2, "pymab.policies.policy.no_context_func", false]], "optimal_actions (game attribute)": [[0, "pymab.game.Game.optimal_actions", false]], "optimistic_initialization (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.optimistic_initialization", false]], "optimistic_initialization (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.optimistic_initialization", false]], "optimistic_initialization (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.optimistic_initialization", false]], "optimistic_initialization (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.optimistic_initialization", false]], "optimistic_initialization (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.optimistic_initialization", false]], "optimistic_initialization (policy attribute)": [[2, "pymab.policies.policy.Policy.optimistic_initialization", false]], "optimistic_initialization (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.optimistic_initialization", false]], "optimistic_initialization (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.optimistic_initialization", false]], "optimistic_initialization (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.optimistic_initialization", false]], "plot_average_reward_by_episode() (game method)": [[0, "pymab.game.Game.plot_average_reward_by_episode", false]], "plot_average_reward_by_step() (game method)": [[0, "pymab.game.Game.plot_average_reward_by_step", false]], "plot_average_reward_by_step_smoothed() (game method)": [[0, "pymab.game.Game.plot_average_reward_by_step_smoothed", false]], "plot_bandit_selection_evolution() (game method)": [[0, "pymab.game.Game.plot_bandit_selection_evolution", false]], "plot_cumulative_regret_by_step() (game method)": [[0, "pymab.game.Game.plot_cumulative_regret_by_step", false]], "plot_distribution() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.plot_distribution", false]], "plot_distribution() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.plot_distribution", false]], "plot_distribution() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.plot_distribution", false]], "plot_distribution() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.plot_distribution", false]], "plot_distribution() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.plot_distribution", false]], "plot_distribution() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.plot_distribution", false]], "plot_distribution() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.plot_distribution", false]], "plot_distribution() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy.plot_distribution", false]], "plot_distribution() (policy method)": [[2, "pymab.policies.policy.Policy.plot_distribution", false]], "plot_distribution() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.plot_distribution", false]], "plot_distribution() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.plot_distribution", false]], "plot_distribution() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy.plot_distribution", false]], "plot_q_values() (game method)": [[0, "pymab.game.Game.plot_Q_values", false]], "plot_q_values_evolution_by_bandit_first_episode() (game method)": [[0, "pymab.game.Game.plot_Q_values_evolution_by_bandit_first_episode", false]], "plot_rate_optimal_actions_by_step() (game method)": [[0, "pymab.game.Game.plot_rate_optimal_actions_by_step", false]], "plot_total_reward_by_step() (game method)": [[0, "pymab.game.Game.plot_total_reward_by_step", false]], "policies (game attribute)": [[0, "pymab.game.Game.policies", false]], "policy (class in pymab.policies.policy)": [[2, "pymab.policies.policy.Policy", false]], "precisions (gaussianthompsonsamplingpolicy attribute)": [[2, "id39", false], [2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.precisions", false]], "pymab.game": [[0, "module-pymab.game", false]], "pymab.policies.bayesian_ucb": [[2, "module-pymab.policies.bayesian_ucb", false]], "pymab.policies.contextual_bandits": [[2, "module-pymab.policies.contextual_bandits", false]], "pymab.policies.epsilon_greedy": [[2, "module-pymab.policies.epsilon_greedy", false]], "pymab.policies.gradient": [[2, "module-pymab.policies.gradient", false]], "pymab.policies.greedy": [[2, "module-pymab.policies.greedy", false]], "pymab.policies.policy": [[2, "module-pymab.policies.policy", false]], "pymab.policies.softmax_selection": [[2, "module-pymab.policies.softmax_selection", false]], "pymab.policies.thompson_sampling": [[2, "module-pymab.policies.thompson_sampling", false]], "pymab.policies.ucb": [[2, "module-pymab.policies.ucb", false]], "pymab.reward_distribution": [[3, "module-pymab.reward_distribution", false]], "q_values (bernoullibayesianucbpolicy property)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.Q_values", false]], "q_values (bernoullithompsonsamplingpolicy property)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.Q_values", false]], "q_values (contextualbanditpolicy property)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.Q_values", false]], "q_values (discounteducbpolicy property)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.Q_values", false]], "q_values (epsilongreedypolicy property)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.Q_values", false]], "q_values (game attribute)": [[0, "pymab.game.Game.Q_values", false]], "q_values (gaussianbayesianucbpolicy property)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.Q_values", false]], "q_values (gaussianthompsonsamplingpolicy property)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.Q_values", false]], "q_values (greedypolicy property)": [[2, "pymab.policies.greedy.GreedyPolicy.Q_values", false]], "q_values (policy property)": [[2, "pymab.policies.policy.Policy.Q_values", false]], "q_values (slidingwindowucbpolicy property)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.Q_values", false]], "q_values (stationaryucbpolicy property)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.Q_values", false]], "q_values (ucbpolicy property)": [[2, "pymab.policies.ucb.UCBPolicy.Q_values", false]], "q_values_history (game attribute)": [[0, "pymab.game.Game.Q_values_history", false]], "random_arm_swapping (environmentchangetype attribute)": [[0, "pymab.game.EnvironmentChangeType.RANDOM_ARM_SWAPPING", false]], "randomarmswappingenvironmentmixin (class in pymab.game)": [[0, "pymab.game.RandomArmSwappingEnvironmentMixin", false]], "regret_by_policy (game attribute)": [[0, "pymab.game.Game.regret_by_policy", false]], "reset() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.reset", false]], "reset() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.reset", false]], "reset() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.reset", false]], "reset() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.reset", false]], "reset() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.reset", false]], "reset() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.reset", false]], "reset() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.reset", false]], "reset() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy.reset", false]], "reset() (policy method)": [[2, "pymab.policies.policy.Policy.reset", false]], "reset() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.reset", false]], "reset() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.reset", false]], "reset() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy.reset", false]], "reward_distribution (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.reward_distribution", false]], "reward_distribution (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.reward_distribution", false]], "reward_distribution (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.reward_distribution", false]], "reward_distribution (game attribute)": [[0, "pymab.game.Game.reward_distribution", false]], "reward_distribution (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.reward_distribution", false]], "reward_distribution (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.reward_distribution", false]], "reward_distribution (policy attribute)": [[2, "pymab.policies.policy.Policy.reward_distribution", false]], "reward_distribution (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.reward_distribution", false]], "reward_distribution (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.reward_distribution", false]], "reward_distribution (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.reward_distribution", false]], "rewarddistribution (class in pymab.reward_distribution)": [[3, "pymab.reward_distribution.RewardDistribution", false]], "rewards_by_policy (game attribute)": [[0, "pymab.game.Game.rewards_by_policy", false]], "rewards_history (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.rewards_history", false]], "rewards_history (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.rewards_history", false]], "rewards_history (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.rewards_history", false]], "rewards_history (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.rewards_history", false]], "rewards_history (policy attribute)": [[2, "pymab.policies.policy.Policy.rewards_history", false]], "rewards_history (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.rewards_history", false]], "rewards_history (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.rewards_history", false]], "rewards_history (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.rewards_history", false]], "select_action() (bernoullibayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.select_action", false]], "select_action() (bernoullithompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.select_action", false]], "select_action() (contextualbanditpolicy method)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.select_action", false]], "select_action() (discounteducbpolicy method)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.select_action", false]], "select_action() (epsilongreedypolicy method)": [[2, "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy.select_action", false]], "select_action() (gaussianbayesianucbpolicy method)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.select_action", false]], "select_action() (gaussianthompsonsamplingpolicy method)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.select_action", false]], "select_action() (greedypolicy method)": [[2, "pymab.policies.greedy.GreedyPolicy.select_action", false]], "select_action() (policy method)": [[2, "pymab.policies.policy.Policy.select_action", false]], "select_action() (slidingwindowucbpolicy method)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.select_action", false]], "select_action() (stationaryucbpolicy method)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.select_action", false]], "select_action() (ucbpolicy method)": [[2, "pymab.policies.ucb.UCBPolicy.select_action", false]], "slidingwindowucbpolicy (class in pymab.policies.ucb)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy", false]], "stationary (environmentchangetype attribute)": [[0, "pymab.game.EnvironmentChangeType.STATIONARY", false]], "stationaryenvironmentmixin (class in pymab.game)": [[0, "pymab.game.StationaryEnvironmentMixin", false]], "stationaryucbpolicy (class in pymab.policies.ucb)": [[2, "pymab.policies.ucb.StationaryUCBPolicy", false]], "successes (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.successes", false]], "successes (bernoullithompsonsamplingpolicy attribute)": [[2, "id0", false], [2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.successes", false]], "sum_rewards (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.sum_rewards", false]], "sum_squared_rewards (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.sum_squared_rewards", false]], "theta (contextualbanditpolicy attribute)": [[2, "pymab.policies.contextual_bandits.ContextualBanditPolicy.theta", false]], "thompsonsamplingpolicy (class in pymab.policies.thompson_sampling)": [[2, "pymab.policies.thompson_sampling.ThompsonSamplingPolicy", false]], "thomson_sampled (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.thomson_sampled", false]], "times_selected (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.times_selected", false]], "times_selected (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.times_selected", false]], "times_selected (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.times_selected", false]], "times_selected (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.times_selected", false]], "times_selected (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.times_selected", false]], "times_selected (policy attribute)": [[2, "pymab.policies.policy.Policy.times_selected", false]], "times_selected (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.times_selected", false]], "times_selected (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.times_selected", false]], "times_selected (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.times_selected", false]], "total_reward (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.total_reward", false]], "total_reward (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.total_reward", false]], "total_reward (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.total_reward", false]], "total_reward (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.total_reward", false]], "total_reward (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.total_reward", false]], "total_reward (policy attribute)": [[2, "pymab.policies.policy.Policy.total_reward", false]], "total_reward (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.total_reward", false]], "total_reward (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.total_reward", false]], "total_reward (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.total_reward", false]], "total_rewards_by_step (game property)": [[0, "pymab.game.Game.total_rewards_by_step", false]], "ucbpolicy (class in pymab.policies.ucb)": [[2, "pymab.policies.ucb.UCBPolicy", false]], "uniformrewarddistribution (class in pymab.reward_distribution)": [[3, "pymab.reward_distribution.UniformRewardDistribution", false]], "variance (bernoullibayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy.variance", false]], "variance (bernoullithompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.variance", false]], "variance (discounteducbpolicy attribute)": [[2, "pymab.policies.ucb.DiscountedUCBPolicy.variance", false]], "variance (gaussianbayesianucbpolicy attribute)": [[2, "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy.variance", false]], "variance (gaussianthompsonsamplingpolicy attribute)": [[2, "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.variance", false]], "variance (policy attribute)": [[2, "pymab.policies.policy.Policy.variance", false]], "variance (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.variance", false]], "variance (stationaryucbpolicy attribute)": [[2, "pymab.policies.ucb.StationaryUCBPolicy.variance", false]], "variance (ucbpolicy attribute)": [[2, "pymab.policies.ucb.UCBPolicy.variance", false]], "window_size (slidingwindowucbpolicy attribute)": [[2, "pymab.policies.ucb.SlidingWindowUCBPolicy.window_size", false]]}, "objects": {"pymab": [[0, 0, 0, "-", "game"], [3, 0, 0, "-", "reward_distribution"]], "pymab.game": [[0, 1, 1, "", "AbruptChangeEnvironmentMixin"], [0, 1, 1, "", "EnvironmentChangeMixin"], [0, 1, 1, "", "EnvironmentChangeType"], [0, 1, 1, "", "Game"], [0, 1, 1, "", "GradualChangeEnvironmentMixin"], [0, 1, 1, "", "RandomArmSwappingEnvironmentMixin"], [0, 1, 1, "", "StationaryEnvironmentMixin"]], "pymab.game.AbruptChangeEnvironmentMixin": [[0, 2, 1, "", "__init__"], [0, 2, 1, "", "apply_change"]], "pymab.game.EnvironmentChangeMixin": [[0, 2, 1, "", "apply_change"]], "pymab.game.EnvironmentChangeType": [[0, 3, 1, "", "ABRUPT"], [0, 3, 1, "", "GRADUAL"], [0, 3, 1, "", "RANDOM_ARM_SWAPPING"], [0, 3, 1, "", "STATIONARY"]], "pymab.game.Game": [[0, 3, 1, "", "Q_values"], [0, 3, 1, "", "Q_values_history"], [0, 2, 1, "", "__init__"], [0, 3, 1, "", "actions_selected_by_policy"], [0, 4, 1, "", "average_rewards_by_episode"], [0, 4, 1, "", "average_rewards_by_step"], [0, 4, 1, "", "cumulative_regret_by_step"], [0, 2, 1, "", "game_loop"], [0, 3, 1, "", "n_episodes"], [0, 3, 1, "", "n_steps"], [0, 2, 1, "", "new_episode"], [0, 3, 1, "", "optimal_actions"], [0, 2, 1, "", "plot_Q_values"], [0, 2, 1, "", "plot_Q_values_evolution_by_bandit_first_episode"], [0, 2, 1, "", "plot_average_reward_by_episode"], [0, 2, 1, "", "plot_average_reward_by_step"], [0, 2, 1, "", "plot_average_reward_by_step_smoothed"], [0, 2, 1, "", "plot_bandit_selection_evolution"], [0, 2, 1, "", "plot_cumulative_regret_by_step"], [0, 2, 1, "", "plot_rate_optimal_actions_by_step"], [0, 2, 1, "", "plot_total_reward_by_step"], [0, 3, 1, "", "policies"], [0, 3, 1, "", "regret_by_policy"], [0, 3, 1, "", "reward_distribution"], [0, 3, 1, "", "rewards_by_policy"], [0, 4, 1, "", "total_rewards_by_step"]], "pymab.game.GradualChangeEnvironmentMixin": [[0, 2, 1, "", "__init__"], [0, 2, 1, "", "apply_change"]], "pymab.game.RandomArmSwappingEnvironmentMixin": [[0, 2, 1, "", "__init__"], [0, 2, 1, "", "apply_change"]], "pymab.game.StationaryEnvironmentMixin": [[0, 2, 1, "", "apply_change"]], "pymab.policies": [[2, 0, 0, "-", "bayesian_ucb"], [2, 0, 0, "-", "contextual_bandits"], [2, 0, 0, "-", "epsilon_greedy"], [2, 0, 0, "-", "gradient"], [2, 0, 0, "-", "greedy"], [2, 0, 0, "-", "policy"], [2, 0, 0, "-", "softmax_selection"], [2, 0, 0, "-", "thompson_sampling"], [2, 0, 0, "-", "ucb"]], "pymab.policies.bayesian_ucb": [[2, 1, 1, "", "BayesianUCBPolicy"], [2, 1, 1, "", "BernoulliBayesianUCBPolicy"], [2, 1, 1, "", "GaussianBayesianUCBPolicy"]], "pymab.policies.bayesian_ucb.BayesianUCBPolicy": [[2, 2, 1, "", "__new__"]], "pymab.policies.bayesian_ucb.BernoulliBayesianUCBPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_calculate_confidence_interval"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_get_ucb_value"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "c"], [2, 3, 1, "", "current_step"], [2, 3, 1, "", "failures"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "successes"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.bayesian_ucb.GaussianBayesianUCBPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_calculate_confidence_interval"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_get_ucb_value"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "c"], [2, 3, 1, "", "current_step"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "sum_rewards"], [2, 3, 1, "", "sum_squared_rewards"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.contextual_bandits": [[2, 1, 1, "", "ContextualBanditPolicy"]], "pymab.policies.contextual_bandits.ContextualBanditPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "context_dim"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "learning_rate"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "theta"]], "pymab.policies.epsilon_greedy": [[2, 1, 1, "", "EpsilonGreedyPolicy"]], "pymab.policies.epsilon_greedy.EpsilonGreedyPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "epsilon"], [2, 2, 1, "", "get_reward_distribution"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 2, 1, "", "select_action"]], "pymab.policies.greedy": [[2, 1, 1, "", "GreedyPolicy"]], "pymab.policies.greedy.GreedyPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 2, 1, "", "get_reward_distribution"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 2, 1, "", "select_action"]], "pymab.policies.policy": [[2, 1, 1, "", "Policy"], [2, 5, 1, "", "no_context_func"]], "pymab.policies.policy.Policy": [[2, 4, 1, "", "Q_values"], [2, 3, 1, "", "_Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_estimate"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "context_func"], [2, 3, 1, "", "current_step"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.thompson_sampling": [[2, 1, 1, "", "BernoulliThompsonSamplingPolicy"], [2, 1, 1, "", "GaussianThompsonSamplingPolicy"], [2, 1, 1, "", "ThompsonSamplingPolicy"]], "pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "current_step"], [2, 3, 1, "id29", "failures"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "id0", "successes"], [2, 3, 1, "", "thomson_sampled"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "current_step"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "id38", "means"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 3, 1, "id39", "precisions"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.thompson_sampling.ThompsonSamplingPolicy": [[2, 2, 1, "", "__new__"]], "pymab.policies.ucb": [[2, 1, 1, "", "DiscountedUCBPolicy"], [2, 1, 1, "", "SlidingWindowUCBPolicy"], [2, 1, 1, "", "StationaryUCBPolicy"], [2, 1, 1, "", "UCBPolicy"]], "pymab.policies.ucb.DiscountedUCBPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_calculate_confidence_interval"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_get_ucb_value"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "c"], [2, 3, 1, "", "current_step"], [2, 3, 1, "", "discount_factor"], [2, 3, 1, "", "effective_n"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.ucb.SlidingWindowUCBPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_calculate_confidence_interval"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_get_ucb_value"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "c"], [2, 3, 1, "", "current_step"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"], [2, 3, 1, "", "window_size"]], "pymab.policies.ucb.StationaryUCBPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_calculate_confidence_interval"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_get_ucb_value"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "c"], [2, 3, 1, "", "current_step"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.policies.ucb.UCBPolicy": [[2, 4, 1, "", "Q_values"], [2, 2, 1, "", "__init__"], [2, 2, 1, "", "_calculate_confidence_interval"], [2, 2, 1, "", "_get_actual_reward"], [2, 2, 1, "", "_get_ucb_value"], [2, 2, 1, "", "_update"], [2, 2, 1, "", "_update_sliding_window"], [2, 3, 1, "", "actions_estimated_reward"], [2, 3, 1, "", "c"], [2, 3, 1, "", "current_step"], [2, 2, 1, "", "get_reward_distribution"], [2, 3, 1, "", "n_bandits"], [2, 3, 1, "", "optimistic_initialization"], [2, 2, 1, "", "plot_distribution"], [2, 2, 1, "", "reset"], [2, 3, 1, "", "reward_distribution"], [2, 3, 1, "", "rewards_history"], [2, 2, 1, "", "select_action"], [2, 3, 1, "", "times_selected"], [2, 3, 1, "", "total_reward"], [2, 3, 1, "", "variance"]], "pymab.reward_distribution": [[3, 1, 1, "", "BernoulliRewardDistribution"], [3, 1, 1, "", "GaussianRewardDistribution"], [3, 1, 1, "", "RewardDistribution"], [3, 1, 1, "", "UniformRewardDistribution"]], "pymab.reward_distribution.BernoulliRewardDistribution": [[3, 2, 1, "", "generate_Q_values"], [3, 2, 1, "", "get_reward"]], "pymab.reward_distribution.GaussianRewardDistribution": [[3, 2, 1, "", "generate_Q_values"], [3, 2, 1, "", "get_reward"]], "pymab.reward_distribution.RewardDistribution": [[3, 2, 1, "", "generate_Q_values"], [3, 2, 1, "", "get_reward"]], "pymab.reward_distribution.UniformRewardDistribution": [[3, 2, 1, "", "generate_Q_values"], [3, 2, 1, "", "get_reward"]]}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "property", "Python property"], "5": ["py", "function", "Python function"]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:property", "5": "py:function"}, "terms": {"": [0, 2], "0": [0, 2, 3], "01": [0, 2], "1": [0, 1, 2, 3], "10": [0, 2], "100": [0, 2], "1000": [0, 1, 2], "2": [0, 2, 3], "200": 2, "2000": [0, 1], "3": [2, 3], "30": 3, "4": 2, "5": [0, 1, 2, 3], "50": 0, "68": 3, "7": 3, "70": 3, "9": 2, "95": 3, "99": 3, "A": [2, 3], "For": 0, "If": [0, 2, 3], "It": [0, 2], "No": 2, "Not": 3, "The": [0, 1, 2, 3], "These": 2, "_": 2, "__init__": [0, 2], "__new__": 2, "_calculate_confidence_interv": 2, "_get_actual_reward": 2, "_get_ucb_valu": 2, "_q_valu": 2, "_updat": 2, "_update_estim": 2, "_update_sliding_window": 2, "abc": [0, 2, 3], "about": 2, "abrupt": 0, "abruptchangeenvironmentmixin": 0, "abruptli": 0, "abstract": [0, 2, 3], "accord": 2, "accur": 2, "action": [0, 2], "action_index": 2, "actions_estimated_reward": 2, "actions_selected_by_polici": 0, "actual": 2, "ad": [0, 2], "adapt": 2, "add": 2, "addit": 2, "address": 2, "advanc": 2, "affect": 0, "after": [0, 2], "ag": 2, "agent": 2, "algorithm": [0, 1, 2], "all": [0, 2, 3], "allow": [1, 2, 3], "also": 2, "altern": 2, "alwai": [0, 2], "an": [0, 1, 2], "analyz": 0, "ani": [0, 2], "append": 3, "appli": 0, "apply_chang": 0, "approach": 2, "appropri": 2, "approxim": 3, "ar": [0, 2], "arbitrari": 2, "arg": 2, "argument": 2, "arm": [0, 1, 2, 3], "around": 3, "arrai": [0, 2], "assum": 2, "attribut": 2, "avail": 2, "averag": 2, "average_rewards_by_episod": 0, "average_rewards_by_step": 0, "avoid": 2, "balanc": 2, "bandit": [0, 1, 3], "base": [0, 2, 3], "basic": 2, "bayesian": 1, "bayesian_ucb": 1, "bayesianucbpolici": 2, "been": [0, 2], "behavior": [0, 2], "being": [0, 2, 3], "belief": 2, "below": 2, "bernoulli": [0, 2, 3], "bernoullibayesianucbpolici": 2, "bernoullidistribut": 3, "bernoullirewarddistribut": 3, "bernoullithompsonsamplingpolici": 2, "best": [0, 2], "beta": 2, "better": 2, "between": [0, 2, 3], "binari": 2, "bool": 0, "both": [2, 3], "bound": 2, "built": 0, "c": 2, "calcul": [0, 2], "call": 0, "callabl": 2, "can": [0, 2], "capit": 2, "case": 2, "caus": 2, "center": 3, "central": 3, "certain": 2, "chang": [0, 2], "change_frequ": 0, "change_magnitud": 0, "change_param": 0, "change_r": 0, "characterist": 2, "choic": 2, "choos": 2, "chosen": [0, 2], "chosen_action_index": 2, "cl": 2, "class": [0, 2, 3], "close": 2, "closer": 2, "coeffici": 2, "collect": 0, "combin": 2, "common": 2, "compar": [0, 1], "compat": 3, "compon": [0, 3], "comprehens": 0, "comput": 2, "computation": 2, "condit": 3, "confid": 2, "configur": [0, 1], "conjug": 2, "consid": 2, "constructor": 2, "contain": [0, 2], "context": 2, "context_chosen_act": 2, "context_dim": 2, "context_func": 2, "contextu": 1, "contextual_bandit": 1, "contextualbanditpolici": 2, "continu": 2, "control": 2, "converg": 2, "core": [0, 2], "correspond": 2, "count": 2, "counter": 2, "creat": [2, 3], "crucial": 3, "cumul": [0, 2], "cumulative_regret_by_step": 0, "current": 2, "current_step": 2, "custom": 0, "customiz": 3, "dai": 2, "data": [0, 2, 3], "decid": 2, "decis": 2, "def": 2, "default": [0, 2], "default_results_fold": 0, "defin": [1, 2, 3], "degre": 2, "depend": 2, "design": 1, "detail": 2, "determin": 2, "determinist": 2, "deviat": [0, 3], "dict": 0, "differ": [0, 1, 3], "dilemma": 2, "dimens": 2, "discount": 2, "discount_factor": 2, "discountedmixin": 2, "discounteducbpolici": 2, "discov": 2, "distribut": [0, 1, 2], "divers": 3, "do": 2, "doe": 2, "don": 2, "dot": 2, "drawn": [0, 2], "drop": 2, "dummi": 2, "dynam": 0, "e": [2, 3], "each": [0, 2, 3], "easi": 1, "effect": 2, "effective_n": 2, "effici": 2, "either": 2, "empir": 2, "enabl": 2, "encourag": 2, "ensur": 2, "enum": 0, "enumer": 0, "environ": [0, 2, 3], "environment_chang": 0, "environmentchangemixin": 0, "environmentchangetyp": 0, "episod": 0, "episode_idx": 0, "epsilon": [0, 1], "epsilon_greedi": [1, 3], "epsilon_greedy_policy_0_01": 2, "epsilon_greedy_policy_0_1": 2, "epsilon_greedy_policy_0_5": 2, "epsilongreedypolici": [0, 2, 3], "equal": 3, "equival": 2, "especi": 2, "estim": 2, "evalu": 0, "everi": [0, 3], "expect": 2, "experi": [0, 1], "explicit": 2, "explicitli": 2, "exploit": [1, 2], "explor": [1, 2], "exploratori": 1, "extend": 2, "factor": 2, "factori": 2, "fail": 2, "failur": 2, "fast": 2, "featur": [0, 2, 3], "find": 2, "first": 2, "fix": 2, "flexibl": [0, 1], "float": [0, 2, 3], "focus": 2, "follow": 2, "form": 2, "formula": 2, "foundat": 3, "framework": [0, 1, 2], "from": [0, 1, 2, 3], "full": 2, "function": 2, "g": 3, "game": [1, 2, 3], "game_loop": [0, 1, 2], "gather": 2, "gaussian": [2, 3], "gaussianbayesianucbpolici": 2, "gaussiandistribut": 3, "gaussianrewarddistribut": 3, "gaussianthompsonsamplingpolici": 2, "gener": [0, 2, 3], "generate_q_valu": 3, "get": [0, 2, 3], "get_reward": 3, "get_reward_distribut": 2, "get_user_context": 2, "give": 2, "given": 2, "gradient": 1, "gradual": 0, "gradualchangeenvironmentmixin": 0, "greedi": [0, 1], "greedy_polici": [1, 2], "greedy_policy_optimistic_initialization_1": 2, "greedy_policy_optimistic_initialization_5": 2, "greedypolici": [0, 1, 2], "guarante": 2, "guassian": 2, "ha": [0, 2, 3], "had": 2, "half": 3, "handl": 2, "have": [0, 2], "haven": 2, "help": 2, "here": 2, "high": 2, "higher": 2, "highest": 2, "histor": 2, "histori": [0, 2], "home": 0, "how": [0, 2], "i": [0, 1, 2, 3], "immedi": 2, "implement": [2, 3], "import": [0, 1, 2, 3], "inaccur": 2, "includ": [0, 2, 3], "incorpor": 2, "increment": 2, "incur": 0, "index": [1, 2], "infer": 2, "infin": 2, "inform": 2, "inherit": 2, "initi": [0, 2], "instanc": [0, 2], "instead": 2, "int": [0, 2, 3], "interest": 2, "interfac": [2, 3], "interv": 2, "introduc": 2, "invers": 2, "involv": 2, "its": 2, "itself": 2, "kei": [0, 2, 3], "keyword": 2, "knowledg": 2, "known": 2, "kwarg": 2, "last": 2, "learn": 2, "learning_r": 2, "length": 2, "lever": 1, "leverag": 2, "librari": [0, 1, 3], "likelihood": 2, "linear": 2, "list": [0, 2, 3], "local": 2, "locat": 2, "log": 2, "loop": 0, "lower": 2, "mai": 2, "main": 0, "maintain": 2, "make": 2, "map": 2, "match": 2, "math": 2, "matrix": 2, "max_a": 2, "maxim": 2, "mean": [0, 2, 3], "measur": 0, "mechan": 2, "method": [0, 2, 3], "might": 2, "min": 2, "minimum": 2, "miss": 2, "model": [2, 3], "modul": [1, 3], "more": 2, "most": 2, "much": [0, 2], "multi": [0, 1, 2, 3], "multipl": [0, 1], "must": [0, 2, 3], "n": 2, "n_a": 2, "n_bandit": [0, 1, 2], "n_episod": [0, 1, 2], "n_polici": 0, "n_step": [0, 1, 2], "name": 2, "natur": [2, 3], "ndarrai": [0, 2, 3], "need": 2, "never": 0, "new_episod": 0, "next": 2, "no_context_func": 2, "non": [0, 2, 3], "none": [0, 2], "normal": [0, 2, 3], "notion": 2, "np": [0, 2, 3], "number": [0, 2, 3], "numer": 2, "numpi": 2, "object": [0, 2], "observ": 2, "obtain": [0, 2], "off": 2, "offer": 1, "onc": 2, "optim": [0, 2], "optima": 2, "optimal_act": 0, "optimist": 2, "optimistic_initi": [0, 1, 2], "option": 0, "other": 2, "out": 2, "outcom": [2, 3], "outsid": 0, "over": [0, 2], "page": 1, "paramet": [0, 2, 3], "past": 2, "path": 0, "per": [0, 2], "perform": [0, 1, 2], "period": 0, "plot": [1, 2], "plot_average_reward_by_episod": 0, "plot_average_reward_by_step": [0, 1, 2], "plot_average_reward_by_step_smooth": 0, "plot_bandit_selection_evolut": 0, "plot_config": 0, "plot_cumulative_regret_by_step": 0, "plot_distribut": 2, "plot_nam": 0, "plot_q_valu": 0, "plot_q_values_evolution_by_bandit_first_episod": 0, "plot_rate_optimal_actions_by_step": 0, "plot_total_reward_by_step": 0, "polici": [0, 1, 3], "poor": 2, "posit": 2, "posixpath": 0, "possibl": [0, 2], "posterior": 2, "potenti": 2, "precis": 2, "precomput": 2, "predict": 2, "prefer": 2, "prevent": 2, "prior": 2, "probabilist": 2, "probabl": [2, 3], "problem": [2, 3], "process": 2, "product": 2, "promot": 2, "properti": [0, 2], "provid": [0, 2, 3], "pull": 1, "pure": 2, "py": 1, "pymab": [0, 2, 3], "pymultibandit": [0, 2, 3], "python": [0, 1, 2], "q": [0, 2, 3], "q_valu": [0, 2, 3], "q_values_histori": 0, "q_values_mean": 0, "q_values_vari": 0, "quantifi": 0, "quick": 2, "quickli": 1, "r": 2, "rain": 2, "rais": [0, 2], "random": [0, 1, 2], "random_arm_swap": 0, "randomarmswappingenvironmentmixin": 0, "randomli": [0, 2], "rang": [2, 3], "rate": 2, "realist": 3, "receiv": 2, "recent": 2, "recommend": 2, "record": 0, "reduc": 2, "regret": 0, "regret_by_polici": 0, "reinforc": 2, "reiniti": 2, "remain": 2, "remov": 2, "repeat": 2, "repres": 2, "reset": 2, "respons": 3, "result": [0, 1], "results_fold": 0, "return": [0, 2, 3], "reward": [0, 1, 2], "reward_distribut": [0, 1, 2], "rewarddistribut": [0, 2, 3], "rewards_by_polici": 0, "rewards_histori": 2, "right": 1, "risk": 2, "rule": 2, "run": [0, 1, 2], "runner": 0, "same": 0, "sampl": [0, 1, 3], "save": 0, "scale": 2, "scenario": [0, 1, 2, 3], "search": 1, "select": [0, 1], "select_act": 2, "self": 0, "separ": 2, "serv": 2, "set": [1, 2, 3], "setup": 2, "shape": [0, 2], "shift_prob": 0, "should": [0, 2], "show": 2, "sidekick": 1, "similarli": 2, "simpl": 2, "simul": [0, 3], "simultan": 0, "singl": 0, "size": [2, 3], "slide": 2, "slidingwindowmixin": 2, "slidingwindowucbpolici": 2, "slower": 2, "slowli": 2, "smooth_factor": 0, "so": 0, "softmax": 1, "softmax_select": 1, "solut": 2, "solv": 2, "sourc": [0, 2, 3], "specif": 2, "specifi": 2, "spread": 3, "sqrt": 2, "squar": 2, "standard": [0, 2, 3], "state": 2, "static": [2, 3], "stationari": [0, 2, 3], "stationaryenvironmentmixin": 0, "stationarypolicymixin": 2, "stationaryucbpolici": 2, "step": [0, 2], "stochast": 3, "storag": 2, "store": 2, "str": [0, 2], "straightforward": 2, "strategi": [0, 2], "strong": 2, "stuck": 2, "subclass": 2, "suboptim": 2, "success": [2, 3], "suitabl": 2, "sum": 2, "sum_reward": 2, "sum_squared_reward": 2, "support": [0, 2, 3], "swap": 0, "system": 2, "t": 2, "take": 2, "taken": 2, "tame": 1, "tau": 2, "term": 2, "test": [0, 3], "theoret": 2, "theori": 2, "theta": 2, "thi": [0, 2, 3], "thompson": 1, "thompson_sampl": 1, "thompsonsamplingpolici": [1, 2], "thomson_sampl": 2, "through": 2, "time": [0, 2], "times_select": 2, "tool": 0, "total": 2, "total_reward": 2, "total_rewards_by_step": 0, "track": [0, 2], "trade": 2, "tradit": 2, "true": [0, 2], "trusti": 1, "try": 2, "ts_polici": 1, "tupl": 2, "type": [0, 2, 3], "typic": 2, "u": 2, "ucb": 1, "ucb1": 2, "ucb_policy_0": 2, "ucb_policy_1": 2, "ucb_policy_2": 2, "ucbpolici": [0, 2], "uncertainti": 2, "under": 3, "understand": 0, "uniform": [2, 3], "uniformrewarddistribut": 3, "union": [0, 2], "unit": 2, "unknown": 2, "unlik": 2, "unselect": 2, "unsuccess": 3, "unsupport": 2, "until": 2, "up": 1, "updat": [0, 2, 3], "upper": 2, "us": [0, 1, 2, 3], "usag": 1, "user": [1, 2, 3], "valu": [0, 2, 3], "valueerror": [0, 2], "vari": 2, "variabl": 2, "varianc": [0, 2, 3], "variant": 2, "varieti": 1, "variou": [0, 2, 3], "vector": 2, "visual": [0, 2], "wa": 2, "weight": 2, "when": 2, "where": [0, 2], "whether": 2, "which": [2, 3], "while": 2, "width": 2, "wild": 1, "window": 2, "window_s": 2, "within": 2, "without": 2, "work": 0, "world": 1, "wors": 0, "would": 0, "yet": 2, "your": 1, "\u03b1": 2, "\u03b2": 2, "\u03b5": 2, "\u03b8": 2, "\u03b8_i": 2, "\u03bc": 2}, "titles": ["Game documentation", "PyMAB documentation", "Policies documentation", "Reward Distribution documentation"], "titleterms": {"bandit": 2, "bayesian": 2, "bayesian_ucb": 2, "content": 1, "contextu": 2, "contextual_bandit": 2, "distribut": 3, "document": [0, 1, 2, 3], "epsilon": 2, "epsilon_greedi": 2, "exampl": [0, 1, 2, 3], "game": 0, "gradient": 2, "greedi": 2, "indic": 1, "polici": 2, "py": [0, 2, 3], "pymab": 1, "reward": 3, "reward_distribut": 3, "sampl": 2, "select": 2, "simpl": 1, "softmax": 2, "softmax_select": 2, "tabl": 1, "thompson": 2, "thompson_sampl": 2, "ucb": 2, "usag": [0, 2, 3]}})