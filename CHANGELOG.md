0.1.0 - 2024-10-31
Added

Initial release of PyMAB
Implementation of basic Multi-Armed Bandit algorithms:

Epsilon-Greedy Policy
Greedy Policy
UCB Policy
Bayesian UCB Policy
Thompson Sampling Policy


Support for different reward distributions:

Gaussian
Bernoulli
Uniform


Support for different environments:
Stationary
Gradual Change
Abrupt Change
Random Arm Swapping


Game class for running bandit simulations
Visualization tools for reward distributions and performance metrics

Changed
None
Deprecated
None
Removed
None
Fixed
None
Security
None