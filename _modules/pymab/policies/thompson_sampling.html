<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>pymab.policies.thompson_sampling &#8212; PyMAB 0.1.0-alpha.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic_mod.css?v=9b2032db" />
    <script src="../../../_static/documentation_options.js?v=6940f236"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <script src="../../../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../../../index.html" title="Go to homepage">PyMAB 0.1.0-alpha.1 documentation</a></h1>

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../../../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper"><p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../policies.html">Policies documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../game.html">Game documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reward_distribution.html">Reward Distribution documentation</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pymab.policies.thompson_sampling</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span><span class="p">,</span> <span class="n">norm</span>

<span class="kn">from</span> <span class="nn">pymab.policies.mixins.stationarity_mixins</span> <span class="kn">import</span> <span class="n">StationaryPolicyMixin</span>
<span class="kn">from</span> <span class="nn">pymab.policies.policy</span> <span class="kn">import</span> <span class="n">Policy</span>


<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">from</span> <span class="nn">pymab.reward_distribution</span> <span class="kn">import</span> <span class="n">RewardDistribution</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">if</span> <span class="n">typing</span><span class="o">.</span><span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="o">*</span>


<div class="viewcode-block" id="BernoulliThompsonSamplingPolicy">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy">[docs]</a>
<span class="k">class</span> <span class="nc">BernoulliThompsonSamplingPolicy</span><span class="p">(</span><span class="n">StationaryPolicyMixin</span><span class="p">,</span> <span class="n">Policy</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This policy is used for multi-armed bandit problems with Bernoulli-distributed rewards.</span>
<span class="sd">    It uses the Beta distribution to model the probability of success for each action and</span>
<span class="sd">    updates these probabilities based on observed rewards.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_bandits: Number of bandit arms available.</span>
<span class="sd">        optimistic_initialization: Initial value for all action estimates. Defaults to 0.</span>
<span class="sd">        variance: Variance of the reward distribution. Defaults to 1.0.</span>
<span class="sd">        reward_distribution: Must be &quot;bernoulli&quot;. Defaults to &quot;bernoulli&quot;.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        successes (np.ndarray): Number of successful outcomes for each arm.</span>
<span class="sd">        failures (np.ndarray): Number of failed outcomes for each arm.</span>
<span class="sd">        thomson_sampled (List[float]): Last sampled values from posterior distributions.</span>

<span class="sd">    Note:</span>
<span class="sd">        Theory:</span>
<span class="sd">        Thompson Sampling implements Bayesian exploration by:</span>
<span class="sd">        1. Maintaining Beta(α, β) posterior for each arm</span>
<span class="sd">        2. α = successes + 1, β = failures + 1 (adding 1 for uniform prior)</span>
<span class="sd">        3. Sampling θ ~ Beta(α, β) for each arm</span>
<span class="sd">        4. Selecting arm with highest sampled θ</span>
<span class="sd">        </span>
<span class="sd">        The Beta distribution is the conjugate prior for Bernoulli likelihood,</span>
<span class="sd">        making updates simple and computationally efficient.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        policy = BernoulliThompsonSamplingPolicy(</span>
<span class="sd">            n_bandits=5,</span>
<span class="sd">            reward_distribution=&quot;bernoulli&quot;</span>
<span class="sd">        )</span>

<span class="sd">        # Run for 1000 steps</span>
<span class="sd">        for _ in range(1000):</span>
<span class="sd">            action, reward = policy.select_action()</span>
<span class="sd">            # Process binary reward (0 or 1)...</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_bandits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">optimistic_initialization</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">_Q_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">total_reward</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">times_selected</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">actions_estimated_reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">reward_distribution</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">RewardDistribution</span><span class="p">]</span>
    <span class="n">rewards_history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span>
    <span class="n">successes</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">failures</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>

<div class="viewcode-block" id="BernoulliThompsonSamplingPolicy.__init__">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">n_bandits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">optimistic_initialization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">reward_distribution</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Policy</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_bandits</span><span class="o">=</span><span class="n">n_bandits</span><span class="p">,</span>
            <span class="n">optimistic_initialization</span><span class="o">=</span><span class="n">optimistic_initialization</span><span class="p">,</span>
            <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span>
            <span class="n">reward_distribution</span><span class="o">=</span><span class="n">reward_distribution</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">successes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">)</span></div>


<div class="viewcode-block" id="BernoulliThompsonSamplingPolicy._update">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy._update">[docs]</a>
    <span class="k">def</span> <span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chosen_action_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the parameters successes and failures used in the Beta distribution, according to the reward</span>
<span class="sd">        obtained.</span>
<span class="sd">        The Bernoulli distribution is conjugate to the Beta distribution, meaning that if the prior distribution of the</span>
<span class="sd">        probability of success is a Beta distribution, then the posterior distribution after observing data is also a</span>
<span class="sd">        Beta distribution. This makes the Bayesian updating process straightforward.</span>

<span class="sd">        Args:</span>
<span class="sd">            chosen_action_index: Index of the chosen action.</span>
<span class="sd">            *args: Variable length argument list.</span>
<span class="sd">            **kwargs: Arbitrary keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The observed reward (0 or 1).</span>

<span class="sd">        Note:</span>
<span class="sd">            Uses conjugate prior property of Beta-Bernoulli:</span>
<span class="sd">            - Success (reward = 1): Increment α</span>
<span class="sd">            - Failure (reward = 0): Increment β</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">chosen_action_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">reward</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">successes</span><span class="p">[</span><span class="n">chosen_action_index</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">failures</span><span class="p">[</span><span class="n">chosen_action_index</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">reward</span></div>


<div class="viewcode-block" id="BernoulliThompsonSamplingPolicy.select_action">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.select_action">[docs]</a>
    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select action using Thompson Sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing:</span>
<span class="sd">                - Index of the chosen action (int)</span>
<span class="sd">                - Reward received for the action (float)</span>

<span class="sd">        Note:</span>
<span class="sd">            Implementation:</span>
<span class="sd">            1. Sample θ ~ Beta(α, β) for each arm</span>
<span class="sd">            2. Select arm with highest θ</span>
<span class="sd">            3. Observe reward and update parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thomson_sampled</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">successes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">failures</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">chosen_action_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thomson_sampled</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">chosen_action_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">chosen_action_index</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span><span class="si">}</span><span class="s2">()&quot;</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="si">{</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span><span class="si">}</span><span class="s2">(</span>
<span class="s2">                    n_bandits=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    Q_values=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_values</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    variance=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">variance</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    successes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">successes</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    failures=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failures</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BernoulliThompsonSamplingPolicy.plot_distribution">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.BernoulliThompsonSamplingPolicy.plot_distribution">[docs]</a>
    <span class="k">def</span> <span class="nf">plot_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plots the distributions of the expected reward for the current step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">x_range</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">BernoulliThompsonSamplingPolicy</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">):</span>
            <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">successes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">failures</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Arm </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> Posterior&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Q_values</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Reward&quot;</span>
            <span class="p">)</span>

            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Arm </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> - Step </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">current_step</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
                <span class="mf">0.5</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;Successes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">successes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, Failures: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failures</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span>
                <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
                <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reward distribution for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></div>
</div>



<div class="viewcode-block" id="GaussianThompsonSamplingPolicy">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy">[docs]</a>
<span class="k">class</span> <span class="nc">GaussianThompsonSamplingPolicy</span><span class="p">(</span><span class="n">StationaryPolicyMixin</span><span class="p">,</span> <span class="n">Policy</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This policy is used for multi-armed bandit problems with Gaussian-distributed rewards.</span>
<span class="sd">    It models the mean reward for each action using a Gaussian distribution and updates</span>
<span class="sd">    these means based on observed rewards.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_bandits: Number of bandit arms available.</span>
<span class="sd">        optimistic_initialization: Initial value for all action estimates. Defaults to 0.</span>
<span class="sd">        variance: Variance of the reward distribution. Defaults to 1.0.</span>
<span class="sd">        reward_distribution: Must be &quot;gaussian&quot;. Defaults to &quot;gaussian&quot;.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        means (np.ndarray): Posterior mean for each arm.</span>
<span class="sd">        precisions (np.ndarray): Posterior precision (1/variance) for each arm.</span>

<span class="sd">    Note:</span>
<span class="sd">        Theory:</span>
<span class="sd">        The policy maintains a Normal distribution for each arm&#39;s mean reward:</span>
<span class="sd">        1. Uses conjugate Normal prior with known variance</span>
<span class="sd">        2. Updates posterior mean and precision after each observation</span>
<span class="sd">        3. Samples from posterior and selects highest sample</span>
<span class="sd">        </span>
<span class="sd">        The Normal distribution is conjugate to itself with known variance,</span>
<span class="sd">        allowing closed-form Bayesian updates.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        policy = GaussianThompsonSamplingPolicy(</span>
<span class="sd">            n_bandits=5,</span>
<span class="sd">            variance=1.0,</span>
<span class="sd">            reward_distribution=&quot;gaussian&quot;</span>
<span class="sd">        )</span>

<span class="sd">        # Run for 1000 steps</span>
<span class="sd">        for _ in range(1000):</span>
<span class="sd">            action, reward = policy.select_action()</span>
<span class="sd">            # Process continuous reward...</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_bandits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">optimistic_initialization</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">_Q_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">total_reward</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">times_selected</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">actions_estimated_reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">reward_distribution</span><span class="p">:</span> <span class="n">RewardDistribution</span>
    <span class="n">means</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>
    <span class="n">precisions</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span>

<div class="viewcode-block" id="GaussianThompsonSamplingPolicy.__init__">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_bandits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">optimistic_initialization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">reward_distribution</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Policy</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_bandits</span><span class="o">=</span><span class="n">n_bandits</span><span class="p">,</span>
            <span class="n">optimistic_initialization</span><span class="o">=</span><span class="n">optimistic_initialization</span><span class="p">,</span>
            <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span>
            <span class="n">reward_distribution</span><span class="o">=</span><span class="n">reward_distribution</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_bandits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precisions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_bandits</span><span class="p">)</span> <span class="o">/</span> <span class="n">variance</span></div>


<div class="viewcode-block" id="GaussianThompsonSamplingPolicy._update">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy._update">[docs]</a>
    <span class="k">def</span> <span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chosen_action_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the Guassian prior distribution according to the observed reward. The conjugate prior for the mean of a Gaussian distribution with known variance is also Gaussian.</span>
<span class="sd">            The posterior distribution of the mean given Gaussian observations remains Gaussian, which allows for a Bayesian update, but it involves maintaining and updating the mean and variance parameters.</span>
<span class="sd">        The means and precisions (tau) arrays maintain the posterior mean and precision (inverse of variance) for each action. These are updated after each observed reward using Bayesian inference.</span>

<span class="sd">        Args:</span>
<span class="sd">            chosen_action_index: Index of the chosen action.</span>
<span class="sd">            *args: Variable length argument list.</span>
<span class="sd">            **kwargs: Arbitrary keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The observed reward (continuous value).</span>

<span class="sd">        Note:</span>
<span class="sd">            Implementation:</span>
<span class="sd">            1. Compute posterior mean using precision-weighted average</span>
<span class="sd">            2. Update precision by adding 1 (for unit variance likelihood)</span>
<span class="sd">            3. Store updated parameters for chosen arm</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">chosen_action_index</span><span class="p">)</span>

        <span class="n">prior_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">chosen_action_index</span><span class="p">]</span>
        <span class="n">prior_precision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">precisions</span><span class="p">[</span><span class="n">chosen_action_index</span><span class="p">]</span>

        <span class="n">posterior_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">prior_precision</span> <span class="o">*</span> <span class="n">prior_mean</span> <span class="o">+</span> <span class="n">reward</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">prior_precision</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">posterior_precision</span> <span class="o">=</span> <span class="n">prior_precision</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">chosen_action_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">posterior_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precisions</span><span class="p">[</span><span class="n">chosen_action_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">posterior_precision</span>

        <span class="k">return</span> <span class="n">reward</span></div>


<div class="viewcode-block" id="GaussianThompsonSamplingPolicy.select_action">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.select_action">[docs]</a>
    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select action using Thompson Sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing:</span>
<span class="sd">                - Index of the chosen action (int)</span>
<span class="sd">                - Reward received for the action (float)</span>

<span class="sd">        Note:</span>
<span class="sd">            Implementation:</span>
<span class="sd">            1. Sample μ ~ N(mean, 1/precision) for each arm</span>
<span class="sd">            2. Select arm with highest sampled μ</span>
<span class="sd">            3. Observe reward and update parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precisions</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">chosen_action_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">chosen_action_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">chosen_action_index</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span><span class="si">}</span><span class="s2">()&quot;</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="si">{</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span><span class="si">}</span><span class="s2">(</span>
<span class="s2">                    n_bandits=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    Q_values=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_values</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    variance=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">variance</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    means=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="si">}</span><span class="se">\n</span>
<span class="s2">                    precisions=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">precisions</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;&quot;&quot;</span>

<div class="viewcode-block" id="GaussianThompsonSamplingPolicy.plot_distribution">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.GaussianThompsonSamplingPolicy.plot_distribution">[docs]</a>
    <span class="k">def</span> <span class="nf">plot_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plots the distributions of the expected reward for the current step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">x_range</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">BernoulliThompsonSamplingPolicy</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bandits</span><span class="p">):</span>
            <span class="n">mean</span><span class="p">,</span> <span class="n">precision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">precisions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">std_dev</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std_dev</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Arm </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> Posterior&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Q_values</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Reward&quot;</span>
            <span class="p">)</span>

            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Arm </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> - Step </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">current_step</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
                <span class="mf">0.5</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;Mean: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">, Precisions: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precisions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span>
                <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
                <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reward distribution for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></div>
</div>



<div class="viewcode-block" id="ThompsonSamplingPolicy">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.ThompsonSamplingPolicy">[docs]</a>
<span class="k">class</span> <span class="nc">ThompsonSamplingPolicy</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Factory class for creating Thompson Sampling policies.</span>

<span class="sd">    This class serves as a factory to create the appropriate Thompson Sampling policy</span>
<span class="sd">    based on the reward distribution type (Bernoulli or Gaussian).</span>

<span class="sd">    Args:</span>
<span class="sd">        n_bandits: Number of bandit arms available.</span>
<span class="sd">        optimistic_initialization: Initial value for all action estimates. Defaults to 0.</span>
<span class="sd">        variance: Variance of the reward distribution. Defaults to 1.0.</span>
<span class="sd">        reward_distribution: Type of reward distribution (&quot;bernoulli&quot; or &quot;gaussian&quot;).</span>
<span class="sd">            Defaults to &quot;gaussian&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The appropriate Thompson Sampling policy instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If an unsupported reward distribution is specified.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        # Create a Bernoulli policy</span>
<span class="sd">        policy = ThompsonSamplingPolicy(</span>
<span class="sd">            n_bandits=5,</span>
<span class="sd">            reward_distribution=&quot;bernoulli&quot;</span>
<span class="sd">        )</span>

<span class="sd">        # Create a Gaussian policy</span>
<span class="sd">        policy = ThompsonSamplingPolicy(</span>
<span class="sd">            n_bandits=5,</span>
<span class="sd">            reward_distribution=&quot;gaussian&quot;</span>
<span class="sd">        )</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="ThompsonSamplingPolicy.__new__">
<a class="viewcode-back" href="../../../policies.html#pymab.policies.thompson_sampling.ThompsonSamplingPolicy.__new__">[docs]</a>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">n_bandits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">optimistic_initialization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">reward_distribution</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">BernoulliThompsonSamplingPolicy</span><span class="p">,</span> <span class="n">GaussianThompsonSamplingPolicy</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">reward_distribution</span> <span class="o">==</span> <span class="s2">&quot;bernoulli&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">BernoulliThompsonSamplingPolicy</span><span class="p">(</span>
                <span class="n">n_bandits</span><span class="o">=</span><span class="n">n_bandits</span><span class="p">,</span>
                <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span>
                <span class="n">reward_distribution</span><span class="o">=</span><span class="n">reward_distribution</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">reward_distribution</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">GaussianThompsonSamplingPolicy</span><span class="p">(</span>
                <span class="n">n_bandits</span><span class="o">=</span><span class="n">n_bandits</span><span class="p">,</span>
                <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span>
                <span class="n">reward_distribution</span><span class="o">=</span><span class="n">reward_distribution</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">reward_distribution</span><span class="si">}</span><span class="s2"> distribution cannot be used with the Thomson Sampling policy!&quot;</span>
            <span class="p">)</span></div>
</div>

</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
            </div>

            <div class="right">
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
    &#169; Copyright 2024, Daniela Lopes.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    </div>

<p id="theme_credit">Styled using the <a href="https://github.com/piccolo-orm/piccolo_theme">Piccolo Theme</a></p>
  </body>
</html>